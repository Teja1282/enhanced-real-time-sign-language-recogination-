## Sign Language Recognition System

## ğŸ“Œ Overview
This project is a **real-time sign language recognition system** that utilizes **deep learning and OpenCV** to interpret hand gestures. A **Convolutional Neural Network (CNN)** is employed to classify gestures, which are then converted into speech using **Google Text-to-Speech (gTTS)**. The system is capable of recognizing American Sign Language (ASL) gestures via a webcam and providing real-time audio feedback.

## ğŸš€ Features
- Load and train the system with an ASL dataset.
- Utilizes a pre-trained CNN model for hand gesture recognition.
- Processes live webcam input for real-time sign detection.
- Converts recognized gestures into speech using gTTS.
- User-friendly interface built with Tkinter.

## ğŸ› ï¸ Technologies Used
- **Programming Language:** Python (Tkinter, OpenCV, NumPy, Keras, gTTS, playsound)
- **Machine Learning:** Deep learning with CNN for gesture classification
- **Computer Vision:** Background subtraction and image processing

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ model/                      # Stores trained model files
â”‚   â”œâ”€â”€ model.json              # CNN model architecture
â”‚   â”œâ”€â”€ model_weights.h5        # CNN model weights
â”‚   â”œâ”€â”€ X.txt.npy, Y.txt.npy    # Dataset features and labels
â”œâ”€â”€ play/                       # Stores generated speech files
â”œâ”€â”€ main.py                     # Main script for GUI & recognition
â”œâ”€â”€ README.md                   # Project documentation
```

## ğŸ”§ Installation
### 1ï¸âƒ£ Prerequisites
Ensure that **Python 3.x** is installed along with the required dependencies:

```sh
pip install numpy opencv-python keras tensorflow gtts playsound tkinter imutils
```

### 2ï¸âƒ£ Clone the Repository
```sh
git clone https://github.com/yourusername/sign-language-recognition.git
cd sign-language-recognition
```

### 3ï¸âƒ£ Run the Application
```sh
python main.py
```

## ğŸ“¸ How It Works
1. **Upload Dataset** â€“ Import ASL gesture images for training.
2. **Train CNN Model** â€“ Trains a deep learning model using the dataset.
3. **Start Webcam** â€“ Captures and processes hand gestures in real time.
4. **Predict Gesture** â€“ Identifies the sign language gesture.
5. **Convert to Speech** â€“ Converts recognized signs into spoken output.

## ğŸ¯ Future Enhancements
- Enhance accuracy by adopting advanced deep learning architectures.
- Expand support to include additional sign languages.
- Integrate **MediaPipe Hands** for improved hand tracking.
- Develop a mobile-compatible version of the application.

## ğŸ† Contributors
- **Your Name** - [GitHub](https://github.com/Teja1282)

## ğŸ“œ License
This project is licensed under the MIT License.

